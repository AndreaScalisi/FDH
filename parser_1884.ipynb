{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the data from the 1884 OCR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = pd.read_excel(\"beau_monde_1884_tables.xlsx\",header = None, sheet_name = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict['table_1'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many rows with missing values (or nan values) do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for table in df_dict:\n",
    "    counter += df_dict[table].isna().sum().sum()\n",
    "print('There are %d missing values in our data.' %(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there a too many values to fill them in by hand, we will just get rid of the corresponding rows (at least for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in df_dict:\n",
    "    df_dict[table].dropna(inplace=True)\n",
    "    \n",
    "counter = 0\n",
    "for table in df_dict:\n",
    "    counter += df_dict[table].isna().sum().sum()\n",
    "print('Now, there are %d missing values in our data.' %(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now harmonize the format of all the dataframes. We want to end up with one dataframe with two colums: Name and Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['Names', 'Adress']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "for table in df_dict:\n",
    "    cnt[str(len(df_dict[table].columns)) + ' columns'] += 1\n",
    "\n",
    "cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be different formats for the dataframes. Let's focus on the most common for now (3, 4 and 5 colums)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = pd.DataFrame(columns = [0,1,2])\n",
    "df_4 = pd.DataFrame(columns = [0,1,2,3])\n",
    "df_5 = pd.DataFrame(columns = [0,1,2,3,4])\n",
    "\n",
    "for table in df_dict:\n",
    "    if len(df_dict[table].columns) == 3: \n",
    "        df_3 = df_3.append(df_dict[table])\n",
    "    elif len(df_dict[table].columns) == 4:\n",
    "        df_4 = df_4.append(df_dict[table])\n",
    "    elif len(df_dict[table].columns) == 5:\n",
    "        df_5 = df_5.append(df_dict[table])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3['Names'] = df_3[0] + ' ' + df_3[1]\n",
    "df_3['Addresses'] = df_3[2]\n",
    "df_3.drop(labels = [0,1,2], axis = 1, inplace = True)\n",
    "df_3.reset_index(inplace = True)\n",
    "df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4['Names'] = df_4[0] + df_4[1] + df_4[2]\n",
    "df_4['Addresses'] = df_4[3]\n",
    "df_4.drop(labels = [0,1,2,3], axis = 1, inplace = True)\n",
    "df_4.reset_index(inplace = True)\n",
    "df_4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4.Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning the addresses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text handling utilities\n",
    "from string import punctuation\n",
    "def lowercase_all(text):\n",
    "    return text.lower()\n",
    "def remove_punct(text):\n",
    "    return ''.join([ch for ch in text if ch not in punctuation])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spelling Corrector based on the work of Peter Norvig: http://norvig.com/spell-correct.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('list_addresses.txt', encoding='utf-8').read())) \n",
    "#list_addresses.txt is a hand-corrected list of addresses\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if (w.isalpha() and w.lower() in WORDS))\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use SpaCy to tokenize the addresses. We just need to add some rules to deal with special cases (like the hyphen in St-Honoré or commas at the end of a word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "infix_re = re.compile(r'''[-~]''') #find hyphens\n",
    "suffix_re = re.compile(r'''[,.\"']$''') #find , or . at end of word\n",
    "def customize_tokenizer(nlp):\n",
    "# Adds support to use `-` as the delimiter for tokenization\n",
    "    return Tokenizer(nlp.vocab, \n",
    "                     infix_finditer=infix_re.finditer,\n",
    "                     suffix_search=suffix_re.search, \n",
    "                     token_match=None)\n",
    "\n",
    "nlp.tokenizer = customize_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define some functions to clean the addresses. The first one corrects spelling error and the second one harmonizes all the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_adrs(adrs):\n",
    "    clean_adrs = ''\n",
    "    \n",
    "    #Tokenize the address using SpaCy tokenizer\n",
    "    adrs_parts = nlp(adrs)\n",
    "    #print ([token.text for token in adrs_parts])\n",
    "    \n",
    "    #Find the street number\n",
    "    last = 1\n",
    "    if str(adrs_parts[-1]) in punctuation:\n",
    "        number = adrs_parts[-2]\n",
    "        last = 2\n",
    "    else:\n",
    "        number = adrs_parts[-1]\n",
    "        \n",
    "    #Correction of errors\n",
    "    for i in range(len(adrs_parts)-last):\n",
    "        if str(adrs_parts[i]) in punctuation:\n",
    "            if str(adrs_parts[i]) == '-':\n",
    "                clean_adrs = clean_adrs[:-1] + str(adrs_parts[i])\n",
    "        else:\n",
    "            clean_adrs += correction(str(adrs_parts[i])).capitalize()\n",
    "            clean_adrs += ' '\n",
    "            \n",
    "    return clean_adrs + str(number)\n",
    "\n",
    "convert_adrs = {'av':'Avenue', \n",
    "        'r':'Rue', \n",
    "        'bd':'Boulevard',\n",
    "        'pl':'Place',\n",
    "        'fr':'Faubourg'}\n",
    "\n",
    "def clean_adrs(adrs):\n",
    "    adrs = correct_adrs(adrs)\n",
    "    adrs_part_punct = adrs.split()\n",
    "    adrs_part = remove_punct(adrs).split()\n",
    "    for i in range(len(adrs_part)):\n",
    "        if lowercase_all(adrs_part[i]) in convert_adrs:\n",
    "            adrs_part_punct[i] = convert_adrs[lowercase_all(adrs_part[i])]\n",
    "    adrs = ' '.join(adrs_part_punct)\n",
    "    return adrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have some tests (still need to get rid of NaN values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_adrs = 'Ff S\\'-llonoré. 21'\n",
    "clean_adrs(test_adrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for adr in df_dict['table_11'][2]:\n",
    "        print(adr + ' : ' + clean_adrs(adr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now clean the addresses in the whole dataframe (FIRST NEED TO HARMONIZE DATAFRAME FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for table in df_dict:\n",
    "    df_table[table]['Address'].apply(clean_adrs, inplace = True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning the names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
