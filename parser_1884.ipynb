{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get the number of elements\n",
    "def count_elmt(df):\n",
    "    return len(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data from the 1884 OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = pd.read_excel(\"beau_monde_1884_tables.xlsx\", header = None, sheet_name = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict['table_1'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count how many addresses we have to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_adr = 0\n",
    "for i in range(167):\n",
    "    table = 'table_' + str(i+1)\n",
    "    num_adr += count_elmt(df_dict[table])\n",
    "    \n",
    "print('At the beginning, we have %d addresses.' %num_adr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many rows with missing values (or nan values) do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for table in df_dict:\n",
    "    counter += df_dict[table].isna().sum().sum()\n",
    "print('There are %d missing values in our data.' %(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there a too many values to fill them in by hand, we will just get rid of the corresponding rows (at least for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in df_dict:\n",
    "    df_dict[table].dropna(inplace=True)\n",
    "    \n",
    "counter = 0\n",
    "for table in df_dict:\n",
    "    counter += df_dict[table].isna().sum().sum()\n",
    "print('Now, there are %d missing values in our data.' %(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now harmonize the format of all the dataframes. We want to end up with one dataframe with two colums: Names and Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "for i in range(167):\n",
    "    table = 'table_' + str(i+1)\n",
    "    cnt[str(len(df_dict[table].columns)) + ' columns'] += 1\n",
    "\n",
    "cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be different formats for the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.DataFrame(columns = [0,1])\n",
    "df_3 = pd.DataFrame(columns = [0,1,2])\n",
    "df_4 = pd.DataFrame(columns = [0,1,2,3])\n",
    "df_5 = pd.DataFrame(columns = [0,1,2,3,4])\n",
    "\n",
    "for i in range(167):\n",
    "    table = 'table_' + str(i+1)\n",
    "    if len(df_dict[table].columns) == 2: \n",
    "        df_2 = df_2.append(df_dict[table])\n",
    "    elif len(df_dict[table].columns) == 3: \n",
    "        df_3 = df_3.append(df_dict[table])\n",
    "    elif len(df_dict[table].columns) == 4:\n",
    "        df_4 = df_4.append(df_dict[table])\n",
    "    elif len(df_dict[table].columns) == 5:\n",
    "        df_5 = df_5.append(df_dict[table])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see those with 2 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.head() #Just names, we can get rid of this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see those with 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3['Names'] = df_3[0] + ' ' + df_3[1]\n",
    "df_3['Addresses'] = df_3[2]\n",
    "df_3.drop(labels = [0,1,2], axis = 1, inplace = True)\n",
    "df_3.reset_index(drop = True, inplace = True)\n",
    "df_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see those with 4 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4['Names'] = df_4[0].map(lambda x: str(x)) + ' ' + df_4[1].map(lambda x: str(x)) + ' ' + df_4[2].map(lambda x: str(x)) \n",
    "df_4['Addresses'] = df_4[3]\n",
    "df_4.drop(labels = [0,1,2,3], axis = 1, inplace = True)\n",
    "df_4.reset_index(drop = True, inplace = True)\n",
    "df_4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see those with 5 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5['Names'] = df_5[0].map(lambda x: str(x)) + ' ' +  df_5[1].map(lambda x: str(x)) + ' ' + df_5[2].map(lambda x: str(x)) \n",
    "df_5['Addresses'] = df_5[3]\n",
    "df_5.drop(labels = [0,1,2,3,4], axis = 1, inplace = True)\n",
    "df_5.reset_index(drop = True, inplace = True)\n",
    "df_5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine them all in one single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_3, df_4, df_5])\n",
    "pre_cleaned = count_elmt(df)\n",
    "num_lost_adr = num_adr - pre_cleaned\n",
    "print('Before cleaning the strings, we have %d addresses.' %pre_cleaned)\n",
    "print('We have therefore lost %d%% addresses due to missing values.' %(100*num_lost_adr/num_adr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/data_to_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_to_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling Corrector\n",
    "\n",
    "Spelling Corrector based on the work of Peter Norvig: http://norvig.com/spell-correct.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text handling utilities\n",
    "from string import punctuation\n",
    "def lowercase_all(text):\n",
    "    return text.lower()\n",
    "def remove_punct(text):\n",
    "    return ''.join([ch for ch in text if ch not in punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('list_addresses.txt', encoding='utf-8').read())) \n",
    "#list_addresses.txt is a hand-corrected list of addresses\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if (w.isalpha() and w.lower() in WORDS))\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use SpaCy to tokenize the addresses. We just need to add some rules to deal with special cases (like the hyphen in St-Honoré or commas at the end of a word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "infix_re = re.compile(r'''[-~]''') #find hyphens\n",
    "suffix_re = re.compile(r'''[,.\"']$''') #find , or . at end of word\n",
    "def customize_tokenizer(nlp):\n",
    "# Adds support to use `-` as the delimiter for tokenization\n",
    "    return Tokenizer(nlp.vocab, \n",
    "                     infix_finditer=infix_re.finditer,\n",
    "                     suffix_search=suffix_re.search, \n",
    "                     token_match=None)\n",
    "\n",
    "nlp.tokenizer = customize_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define some functions to clean the addresses. The first one corrects spelling error and the second one harmonizes all the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_adrs(adrs):\n",
    "    clean_adrs = ''\n",
    "    \n",
    "    #Tokenize the address using SpaCy tokenizer\n",
    "    adrs_parts = nlp(adrs)\n",
    "    #print ([token.text for token in adrs_parts])\n",
    "    \n",
    "    #Find the street number\n",
    "    last = 1\n",
    "    if str(adrs_parts[-1]) in punctuation:\n",
    "        number = adrs_parts[-2]\n",
    "        last = 2\n",
    "    else:\n",
    "        number = adrs_parts[-1]\n",
    "        \n",
    "    #Correction of errors\n",
    "    for i in range(len(adrs_parts)-last):\n",
    "        if adrs_parts[i].text in punctuation:\n",
    "            if adrs_parts[i].text == '-':\n",
    "                clean_adrs = clean_adrs[:-1] + adrs_parts[i].text\n",
    "        else:\n",
    "            clean_adrs += correction(adrs_parts[i].text).capitalize()\n",
    "            clean_adrs += ' '\n",
    "            \n",
    "    return clean_adrs + str(number)\n",
    "\n",
    "convert_adrs = {'av':'Avenue', \n",
    "        'r':'Rue', \n",
    "        'bd':'Boulevard',\n",
    "        'pl':'Place',\n",
    "        'fr':'Faubourg'}\n",
    "\n",
    "def clean_adrs(adrs):\n",
    "    adrs = correct_adrs(adrs)\n",
    "    adrs_part_punct = adrs.split()\n",
    "    adrs_part = remove_punct(adrs).split()\n",
    "    for i in range(len(adrs_part)):\n",
    "        if lowercase_all(adrs_part[i]) in convert_adrs:\n",
    "            adrs_part_punct[i] = convert_adrs[lowercase_all(adrs_part[i])]\n",
    "    adrs = ' '.join(adrs_part_punct)\n",
    "    return adrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution time of this cell is quite long. Directly load the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.Addresses = df.Addresses.apply(clean_adrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/addresses_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('addresses_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some other preprocessing for the addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accent(string):\n",
    "    string = string.replace('é','e')\n",
    "    string = string.replace('è','e')\n",
    "    string = string.replace('ê','e')\n",
    "    string = string.replace('ë','e')\n",
    "    string = string.replace('à','a')\n",
    "    string = string.replace('â','a')\n",
    "    string = string.replace('ô','o')\n",
    "    return string\n",
    "\n",
    "def simplest(string): #Return the simplest form (no punctuation, all lowercase, no accents) of a string\n",
    "    new_string = ''\n",
    "    if type(string) == str:\n",
    "        for c in string:\n",
    "            if c.isalpha():\n",
    "                new_string += c\n",
    "    return remove_punct(lowercase_all(remove_accent(new_string)))\n",
    "\n",
    "def simplest_adr(string): #Format: Avenue St-Honoré 21 -> avenuesthonore21\n",
    "    num = ''\n",
    "    if type(string) == str:\n",
    "        for c in string:\n",
    "            if c.isnumeric():\n",
    "                num += c   \n",
    "    return(simplest(string)+num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Simplest'] = df['Addresses'].apply(simplest_adr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a chunck of those addresses for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proto = df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proto.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paris streets names**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a list of Paris addresses with the corresponding coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord = pd.read_csv('All_nums.csv')\n",
    "coord['Simplest'] = coord['nom_entier'] + coord['num'].map(lambda x: str(x))\n",
    "coord['Simplest'] = coord['Simplest'].apply(simplest_adr)\n",
    "coord.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now merge with the Paris addresses coordinates database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coord = df.merge(coord[['Simplest', 'Y', 'X']], on = 'Simplest')\n",
    "df_coord.drop(labels = ['Unnamed: 0', 'Unnamed: 0.1'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have %d addresses with coordinates.\" %count_elmt(df_coord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coord.to_csv('data/adr_coord.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coord = pd.read_csv('data/adr_coord.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coord.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a second dataframe with all the addresses for which we don't have yet coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_coord = pd.concat([df,df_coord], sort = True).drop_duplicates(subset = 'Simplest', keep = False)\n",
    "df_no_coord.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We still have %d addresses without coordinates.\" %count_elmt(df_no_coord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_coord.to_csv('data/adr_no_coord.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a dict to map all the abbreviations to the corresponding titles (we also include 'Monsieur', 'Madame' and 'Mademoiselle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titres = {'Cte':'Comte', 'Cse':'Comtesse', 'Vte':'Vicomte', 'Vse':'Vicomtesse', 'Dc':'Duc', 'Dse':'Duchesse',\n",
    "         'Bon':'Baron', 'Bne':'Baronne', 'Mis':'Marquis', 'Mse':'Marquise', 'Pce':'Prince', 'M':'Monsieur', 'M°':'Monsieur', \n",
    "          'Me':'Madame', 'Mlle':'Mademoiselle'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: add regex to find patterns of type \"' voyelle\" to get \"d'\" (ex: \"cl ' Adelsward\" doit donner \"d'Adelsward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_name(name):\n",
    "    name_parts = nlp(name)\n",
    "    cleaned_name = ''\n",
    "    titre = ''\n",
    "    #print ([token.text for token in name_parts])\n",
    "    if name_parts[-1].text in punctuation:\n",
    "        name_parts = name_parts[:-1]\n",
    "    for i in range(len(name_parts)):\n",
    "        if name_parts[i].text in titres:\n",
    "            #print(name_parts[i].text)\n",
    "            titre += titres[remove_punct(name_parts[i].text)] #we add the corresponding title using the dict titres\n",
    "            titre += ' '\n",
    "        else: \n",
    "            cleaned_name += name_parts[i].text\n",
    "        cleaned_name += ' '\n",
    "    #print ([token.text for token in name_parts])\n",
    "    \n",
    "    return (titre,cleaned_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'Bon Bne cl\\' Adelsward Gustave'\n",
    "clean_name(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10).Names.apply(clean_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
